{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 train 数据集加载完成: 9831 个文件，类别: {0, 1, 2, 3}\n",
      "📊 test 数据集加载完成: 2808 个文件，类别: {0, 1, 2, 3}\n",
      "📊 validation 数据集加载完成: 1408 个文件，类别: {0, 1, 2, 3}\n",
      "Train 数据集: 9831 文件\n",
      "Test 数据集: 2808 文件\n",
      "Validation 数据集: 1408 文件\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "\n",
    "# 数据集路径\n",
    "dataset_root = \"D:\\\\paper-dataset\\\\mute&cut\"\n",
    "\n",
    "# 4 个类别\n",
    "categories = [\"Tug\", \"Cargo\", \"Passengership\", \"Tanker\"]\n",
    "\n",
    "# 递归加载数据\n",
    "def load_dataset(split):\n",
    "    dataset_path = os.path.join(dataset_root, split)\n",
    "    file_paths, labels = [], []\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise ValueError(f\"错误: 数据集路径 {dataset_path} 不存在!\")\n",
    "\n",
    "    for label, category in enumerate(categories):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"⚠️ 警告: 类别目录 {category_path} 不存在，跳过!\")\n",
    "            continue\n",
    "\n",
    "        for root, _, files in os.walk(category_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_paths.append(os.path.join(root, file))\n",
    "                    labels.append(label)\n",
    "\n",
    "    print(f\"📊 {split} 数据集加载完成: {len(file_paths)} 个文件，类别: {set(labels)}\")\n",
    "    return file_paths, labels\n",
    "\n",
    "# 读取 train, test, validation 数据集\n",
    "train_files, train_labels = load_dataset(\"train\")\n",
    "test_files, test_labels = load_dataset(\"test\")\n",
    "val_files, val_labels = load_dataset(\"validation\")\n",
    "\n",
    "# 打印数据集大小\n",
    "print(f\"Train 数据集: {len(train_files)} 文件\")\n",
    "print(f\"Test 数据集: {len(test_files)} 文件\")\n",
    "print(f\"Validation 数据集: {len(val_files)} 文件\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\lsss3\\anaconda3\\envs\\my_deepship\\Lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "e:\\Users\\lsss3\\anaconda3\\envs\\my_deepship\\Lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    }
   ],
   "source": [
    "# 创建 Preprocessor 实例\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.n_mfcc = 60  # MFCC 维度\n",
    "        self.n_mels = 60  # Mel 频谱图维度\n",
    "        self.sr = 22050   # 采样率\n",
    "\n",
    "    def extract_mfcc(self, waveform):\n",
    "        mfcc_feature = librosa.feature.mfcc(y=waveform, sr=self.sr, n_mfcc=self.n_mfcc, hop_length=512)\n",
    "        return torch.tensor(mfcc_feature)   \n",
    "\n",
    "    def extract_log_mel(self, waveform):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=self.sr, n_mels=self.n_mels, hop_length=512)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        return torch.tensor(log_mel_spectrogram)\n",
    "\n",
    "    def extract_cctz(self, waveform):\n",
    "        chroma = librosa.feature.chroma_stft(y=waveform, sr=self.sr, hop_length=512)\n",
    "        contrast = librosa.feature.spectral_contrast(y=waveform, sr=self.sr, hop_length=512)\n",
    "        tonnetz = librosa.feature.tonnetz(y=waveform, sr=self.sr, hop_length=512)\n",
    "        zero_cross_rate = librosa.feature.zero_crossing_rate(waveform, hop_length=512)\n",
    "\n",
    "        cctz_features = torch.cat([\n",
    "            torch.tensor(chroma), \n",
    "            torch.tensor(contrast), \n",
    "            torch.tensor(tonnetz), \n",
    "            torch.tensor(zero_cross_rate)\n",
    "        ], dim=0)\n",
    "        return cctz_features\n",
    "\n",
    "    def stack_features(self, waveform):\n",
    "        mfcc_feature = self.extract_mfcc(waveform=waveform)\n",
    "        log_mel_feature = self.extract_log_mel(waveform=waveform)\n",
    "        cctz_feature = self.extract_cctz(waveform=waveform)\n",
    "        \n",
    "        max_feature_dim = max(mfcc_feature.size(0), log_mel_feature.size(0), cctz_feature.size(0))\n",
    "\n",
    "        mfcc_tensor = torch.nn.functional.pad(mfcc_feature, (0, 0, 0, max_feature_dim - mfcc_feature.size(0)))\n",
    "        log_mel_tensor = torch.nn.functional.pad(log_mel_feature, (0, 0, 0, max_feature_dim - log_mel_feature.size(0)))\n",
    "        cctz_tensor = torch.nn.functional.pad(cctz_feature, (0, 0, 0, max_feature_dim - cctz_feature.size(0)))\n",
    "        \n",
    "        stacked_features = torch.stack([mfcc_tensor, log_mel_tensor, cctz_tensor], dim=0)\n",
    "        return stacked_features\n",
    "\n",
    "feature_extractor = Preprocessor()\n",
    "\n",
    "def extract_features(file_list):\n",
    "    features = []\n",
    "    for file_path in file_list:\n",
    "        waveform, sr = librosa.load(file_path, sr=feature_extractor.sr)\n",
    "        feature = feature_extractor.stack_features(waveform)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "# 提取特征\n",
    "train_features = extract_features(train_files)\n",
    "test_features = extract_features(test_files)\n",
    "val_features = extract_features(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "class SpecTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpecTransform, self).__init__()\n",
    "        sr = 22050  # 샘플링 주파수 설정\n",
    "        \n",
    "        # 시간 마스킹과 주파수 마스킹 변환 정의\n",
    "        self.time = AT.TimeMasking(time_mask_param=3)  # 시간 마스크 크기 3\n",
    "        self.freq = AT.FrequencyMasking(freq_mask_param=5)  # 주파수 마스크 크기 5\n",
    "\n",
    "    def forward(self, spec):\n",
    "        # 스펙트로그램에 시간 마스크와 주파수 마스크를 적용\n",
    "        spec = self.time(spec)\n",
    "        spec = self.freq(spec)\n",
    "        return spec\n",
    "\n",
    "# 변환 객체 생성\n",
    "spec = SpecTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오디오 데이터셋 클래스 정의\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, label, n_mfcc=60):\n",
    "        self.file_paths = file_paths  # 오디오 파일 경로\n",
    "        self.label = label  # 레이블\n",
    "        self.n_mfcc = n_mfcc  # MFCC 차원 설정\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)  # 데이터셋 크기 반환\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스의 오디오 파일을 가져와서 랜덤 프레임을 추출\n",
    "        audio = self.file_paths[idx]\n",
    "        audio = np.array(audio)\n",
    "        extracted_frame = extract_random_frames(audio)  # 랜덤 프레임 추출\n",
    "        \n",
    "        # 특징 추출\n",
    "        result = feature_extract.stack_features(waveform=extracted_frame)\n",
    "        \n",
    "        # 스펙트로그램에 변환 적용\n",
    "        result[1] = spec(result[1])\n",
    "        \n",
    "        # 레이블 반환\n",
    "        label = int(self.label[idx])\n",
    "        return result.float(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 训练数据集\n",
    "train_dataset = AudioDataset(train_files, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# 验证数据集\n",
    "val_dataset = AudioDataset(val_files, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# 测试数据集\n",
    "test_dataset = AudioDataset(test_files, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_random_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터셋 크기 확인\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# 첫 번째 샘플의 크기 확인\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_paths[idx]\n\u001b[0;32m     16\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(audio)\n\u001b[1;32m---> 17\u001b[0m extracted_frame \u001b[38;5;241m=\u001b[39m \u001b[43mextract_random_frames\u001b[49m(audio)  \u001b[38;5;66;03m# 랜덤 프레임 추출\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# 특징 추출\u001b[39;00m\n\u001b[0;32m     20\u001b[0m result \u001b[38;5;241m=\u001b[39m feature_extract\u001b[38;5;241m.\u001b[39mstack_features(waveform\u001b[38;5;241m=\u001b[39mextracted_frame)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_random_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# 数据集大小检查\n",
    "train_dataset[0][0].shape  # 检查第一个样品的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9831, 1408)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 培训和验证数据集尺寸检查\n",
    "len(train_dataset), len(val_dataset)  # 培训和验证数据集的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        \n",
    "        # 평균 풀링과 최대 풀링을 사용하여 채널 중요도 계산\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # 채널 수를 줄였다가 다시 확장하는 두 개의 1x1 컨볼루션을 사용\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 평균 풀링과 최대 풀링을 통해 각 채널에 대한 중요도를 계산\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        \n",
    "        # 각 채널의 중요도를 합산하고 입력에 가중치를 적용\n",
    "        out = avg_out + max_out\n",
    "        out = out * x\n",
    "        return F.relu(out)  # ReLU 활성화 함수 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1, self).__init__()\n",
    "\n",
    "        # 첫 번째 컨볼루션 + 배치 정규화 + GELU 활성화 함수\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # 채널 어텐션\n",
    "        self.cam1 = ChannelAttention(64)\n",
    "        # 맥스풀링\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 컨볼루션 및 채널 어텐션 적용\n",
    "        out = self.conv1(x)\n",
    "        out = self.cam1(out)\n",
    "        out = self.maxpool(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(AConvBlock, self).__init__()\n",
    "\n",
    "        # 메인 브랜치: 여러 개의 7x7 컨볼루션과 배치 정규화\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels * 8, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(out_channels * 8)\n",
    "        )\n",
    "        \n",
    "        # Shortcut: 입력과 출력의 차원이 다르면 1x1 컨볼루션을 사용해 맞춰줌\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * 8:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 8, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 8)\n",
    "            )\n",
    "        \n",
    "        # 채널 어텐션\n",
    "        self.cam = ChannelAttention(out_channels * 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 메인 브랜치를 통과한 결과\n",
    "        out = self.main_branch(x)\n",
    "        # shortcut 연결 추가\n",
    "        out += self.cam(self.shortcut(x))\n",
    "        return F.relu(out)  # ReLU 활성화 함수 적용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # 네트워크의 각 단계 정의\n",
    "        self.conv1_x = Stage1()\n",
    "        self.conv2_x = AConvBlock(64, 32)  # 입력 채널 64, 출력 채널 32\n",
    "        self.conv3_x = AConvBlock(32*8, 32)\n",
    "        self.conv4_x = AConvBlock(32*8, 32)\n",
    "        self.conv5_x = AConvBlock(32*8, 64)  # 입력 채널 32*8, 출력 채널 64\n",
    "        self.conv6_x = AConvBlock(64*8, 64)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64*8, 24)\n",
    "        self.fc2 = nn.Linear(24, num_classes)\n",
    "\n",
    "        # Average pooling 및 PReLU\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_x(x)\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.conv5_x(out)\n",
    "        out = self.conv6_x(out)\n",
    "        \n",
    "        # Average pooling 후 flatten\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        x = self.prelu(out)\n",
    "        y = self.fc2(x)\n",
    "        \n",
    "        return x, y  # x는 중간 출력, y는 최종 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ResNet().to(device)  # 모델을 지정된 장치로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, feat_dim=256, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # 클래스 중심 초기화\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # 각 클래스 중심과의 거리 계산\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        # 레이블과 일치하는 위치 마스크 생성\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: \n",
    "            classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        # 거리 계산 후 손실 값 구하기\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "# 손실 함수와 CenterLoss 초기화\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # CrossEntropyLoss는 분류 문제에서 사용\n",
    "center = CenterLoss(4, 24).to(device)  # 4개의 클래스, feature dimension 24\n",
    "\n",
    "# 옵티마이저 설정\n",
    "opti1 = Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)  # 모델 파라미터에 대해 Adam 옵티마이저 사용\n",
    "opti2 = SGD(center.parameters(), lr=0.5)  # CenterLoss 파라미터에 대해 SGD 사용\n",
    "\n",
    "# 학습률 스케줄러 설정\n",
    "scheduler1 = torch.optim.lr_scheduler.StepLR(opti1, step_size=10, gamma=0.5)  # 10번째 epoch마다 lr을 0.5배로 감소\n",
    "scheduler2 = torch.optim.lr_scheduler.StepLR(opti2, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, data_len, opti1, opti2):\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    for data, target in tqdm(dataloader):  # 배치 단위로 데이터를 순차적으로 가져옴\n",
    "        data = data.to(device)  # 데이터를 device(CPU/GPU)로 이동\n",
    "        target = target.to(device)  # 타겟 레이블을 device로 이동\n",
    "        \n",
    "        cen, output = model(data)  # 모델의 출력값과 특징 벡터 계산\n",
    "        loss1 = criterion(output, target)  # CrossEntropyLoss 계산\n",
    "        loss2 = center(cen, target)  # CenterLoss 계산\n",
    "        loss = loss1 + loss2  # 두 손실을 합산\n",
    "\n",
    "        opti1.zero_grad()  # 옵티마이저1의 기울기를 0으로 초기화\n",
    "        opti2.zero_grad()  # 옵티마이저2의 기울기를 0으로 초기화\n",
    "        loss.backward()  # 역전파 계산\n",
    "        opti1.step()  # 옵티마이저1 파라미터 업데이트\n",
    "        opti2.step()  # 옵티마이저2 파라미터 업데이트\n",
    "\n",
    "        # 예측값을 구하고 정확도 계산\n",
    "        pred = output.max(1, keepdim=True)[1]  # 예측값의 인덱스 추출\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()  # 정확도 계산\n",
    "        losses += loss.item()  # 손실 값 합산\n",
    "\n",
    "    # 학습률 스케줄러를 업데이트\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "    # 정확도와 평균 손실 반환\n",
    "    return 100 * correct / data_len, losses / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, data_len):\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    with torch.no_grad():  # 평가 시 기울기 계산을 하지 않음\n",
    "        for data, target in dataloader:  # 데이터 로더에서 배치 단위로 데이터를 가져옴\n",
    "            data = data.to(device)  # 데이터를 device로 이동\n",
    "            target = target.to(device)  # 타겟 레이블을 device로 이동\n",
    "\n",
    "            _, output = model(data)  # 모델 출력 계산\n",
    "            loss = criterion(output, target)  # 손실 계산\n",
    "\n",
    "            # 예측값을 구하고 정확도 계산\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = 100. * correct / data_len\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch = 300\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    # Training the model\n",
    "    train_acc, train_loss = train(model, train_dataloader, criterion, len(train_dataloader.dataset), opti1, opti2)\n",
    "    \n",
    "    # Evaluating the model on validation data\n",
    "    val_acc = evaluate(model, val_dataloader, criterion, len(val_dataloader.dataset))\n",
    "    \n",
    "    # Uncomment the line below if you want to evaluate on test data\n",
    "    # test_acc = evaluate(model, test_dataloader, criterion, len(test_dataloader.dataset))\n",
    "\n",
    "    # Storing the accuracies\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Printing the results for the current epoch\n",
    "    print(f\"[Epoch: {i+1}], [Validation Acc: {val_acc:.4f}]\")\n",
    "    print(f\"train_acc: {train_acc}, train_loss: {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Plotting training and validation accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracies')\n",
    "plt.show()\n",
    "\n",
    "# Setting up the device and loading the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet().to(device)\n",
    "model.load_state_dict(torch.load('./deepship7.pt'))\n",
    "\n",
    "# Function to evaluate precision, recall, and F1-score\n",
    "def evaluate_metrics(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            _, output = model(data)\n",
    "            pred = output.argmax(dim=1)  # Get the class with the highest score\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    precision = precision_score(all_targets, all_preds, average=None)\n",
    "    recall = recall_score(all_targets, all_preds, average=None)\n",
    "    f1 = f1_score(all_targets, all_preds, average=None)\n",
    "\n",
    "    # Calculate average precision, recall, and F1-score\n",
    "    avg_precision = precision.mean()\n",
    "    avg_recall = recall.mean()\n",
    "    avg_f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1, avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Evaluate metrics on the test dataset\n",
    "class_precision, class_recall, class_f1, avg_precision, avg_recall, avg_f1 = evaluate_metrics(model, test_dataloader)\n",
    "\n",
    "# Print performance for each class\n",
    "for i in range(len(class_precision)):\n",
    "    print(f\"Class {i} - Precision: {class_precision[i]:.4f}, Recall: {class_recall[i]:.4f}, F1-score: {class_f1[i]:.4f}\")\n",
    "\n",
    "# Print average performance across all classes\n",
    "print(f\"Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1-score: {avg_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_deepship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
