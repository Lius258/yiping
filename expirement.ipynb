{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š train æ•°æ®é›†åŠ è½½å®Œæˆ: 9831 ä¸ªæ–‡ä»¶ï¼Œç±»åˆ«: {0, 1, 2, 3}\n",
      "ğŸ“Š test æ•°æ®é›†åŠ è½½å®Œæˆ: 2808 ä¸ªæ–‡ä»¶ï¼Œç±»åˆ«: {0, 1, 2, 3}\n",
      "ğŸ“Š validation æ•°æ®é›†åŠ è½½å®Œæˆ: 1408 ä¸ªæ–‡ä»¶ï¼Œç±»åˆ«: {0, 1, 2, 3}\n",
      "Train æ•°æ®é›†: 9831 æ–‡ä»¶\n",
      "Test æ•°æ®é›†: 2808 æ–‡ä»¶\n",
      "Validation æ•°æ®é›†: 1408 æ–‡ä»¶\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "\n",
    "\n",
    "# æ•°æ®é›†è·¯å¾„\n",
    "dataset_root = \"D:\\\\paper-dataset\\\\mute&cut\"\n",
    "\n",
    "# 4 ä¸ªç±»åˆ«\n",
    "categories = [\"Tug\", \"Cargo\", \"Passengership\", \"Tanker\"]\n",
    "\n",
    "# é€’å½’åŠ è½½æ•°æ®\n",
    "def load_dataset(split):\n",
    "    dataset_path = os.path.join(dataset_root, split)\n",
    "    file_paths, labels = [], []\n",
    "    \n",
    "    if not os.path.exists(dataset_path):\n",
    "        raise ValueError(f\"é”™è¯¯: æ•°æ®é›†è·¯å¾„ {dataset_path} ä¸å­˜åœ¨!\")\n",
    "\n",
    "    for label, category in enumerate(categories):\n",
    "        category_path = os.path.join(dataset_path, category)\n",
    "\n",
    "        if not os.path.exists(category_path):\n",
    "            print(f\"âš ï¸ è­¦å‘Š: ç±»åˆ«ç›®å½• {category_path} ä¸å­˜åœ¨ï¼Œè·³è¿‡!\")\n",
    "            continue\n",
    "\n",
    "        for root, _, files in os.walk(category_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    file_paths.append(os.path.join(root, file))\n",
    "                    labels.append(label)\n",
    "\n",
    "    print(f\"ğŸ“Š {split} æ•°æ®é›†åŠ è½½å®Œæˆ: {len(file_paths)} ä¸ªæ–‡ä»¶ï¼Œç±»åˆ«: {set(labels)}\")\n",
    "    return file_paths, labels\n",
    "\n",
    "# è¯»å– train, test, validation æ•°æ®é›†\n",
    "train_files, train_labels = load_dataset(\"train\")\n",
    "test_files, test_labels = load_dataset(\"test\")\n",
    "val_files, val_labels = load_dataset(\"validation\")\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†å¤§å°\n",
    "print(f\"Train æ•°æ®é›†: {len(train_files)} æ–‡ä»¶\")\n",
    "print(f\"Test æ•°æ®é›†: {len(test_files)} æ–‡ä»¶\")\n",
    "print(f\"Validation æ•°æ®é›†: {len(val_files)} æ–‡ä»¶\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Users\\lsss3\\anaconda3\\envs\\my_deepship\\Lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "e:\\Users\\lsss3\\anaconda3\\envs\\my_deepship\\Lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»º Preprocessor å®ä¾‹\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.n_mfcc = 60  # MFCC ç»´åº¦\n",
    "        self.n_mels = 60  # Mel é¢‘è°±å›¾ç»´åº¦\n",
    "        self.sr = 22050   # é‡‡æ ·ç‡\n",
    "\n",
    "    def extract_mfcc(self, waveform):\n",
    "        mfcc_feature = librosa.feature.mfcc(y=waveform, sr=self.sr, n_mfcc=self.n_mfcc, hop_length=512)\n",
    "        return torch.tensor(mfcc_feature)   \n",
    "\n",
    "    def extract_log_mel(self, waveform):\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(y=waveform, sr=self.sr, n_mels=self.n_mels, hop_length=512)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "        return torch.tensor(log_mel_spectrogram)\n",
    "\n",
    "    def extract_cctz(self, waveform):\n",
    "        chroma = librosa.feature.chroma_stft(y=waveform, sr=self.sr, hop_length=512)\n",
    "        contrast = librosa.feature.spectral_contrast(y=waveform, sr=self.sr, hop_length=512)\n",
    "        tonnetz = librosa.feature.tonnetz(y=waveform, sr=self.sr, hop_length=512)\n",
    "        zero_cross_rate = librosa.feature.zero_crossing_rate(waveform, hop_length=512)\n",
    "\n",
    "        cctz_features = torch.cat([\n",
    "            torch.tensor(chroma), \n",
    "            torch.tensor(contrast), \n",
    "            torch.tensor(tonnetz), \n",
    "            torch.tensor(zero_cross_rate)\n",
    "        ], dim=0)\n",
    "        return cctz_features\n",
    "\n",
    "    def stack_features(self, waveform):\n",
    "        mfcc_feature = self.extract_mfcc(waveform=waveform)\n",
    "        log_mel_feature = self.extract_log_mel(waveform=waveform)\n",
    "        cctz_feature = self.extract_cctz(waveform=waveform)\n",
    "        \n",
    "        max_feature_dim = max(mfcc_feature.size(0), log_mel_feature.size(0), cctz_feature.size(0))\n",
    "\n",
    "        mfcc_tensor = torch.nn.functional.pad(mfcc_feature, (0, 0, 0, max_feature_dim - mfcc_feature.size(0)))\n",
    "        log_mel_tensor = torch.nn.functional.pad(log_mel_feature, (0, 0, 0, max_feature_dim - log_mel_feature.size(0)))\n",
    "        cctz_tensor = torch.nn.functional.pad(cctz_feature, (0, 0, 0, max_feature_dim - cctz_feature.size(0)))\n",
    "        \n",
    "        stacked_features = torch.stack([mfcc_tensor, log_mel_tensor, cctz_tensor], dim=0)\n",
    "        return stacked_features\n",
    "\n",
    "feature_extractor = Preprocessor()\n",
    "\n",
    "def extract_features(file_list):\n",
    "    features = []\n",
    "    for file_path in file_list:\n",
    "        waveform, sr = librosa.load(file_path, sr=feature_extractor.sr)\n",
    "        feature = feature_extractor.stack_features(waveform)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "# æå–ç‰¹å¾\n",
    "train_features = extract_features(train_files)\n",
    "test_features = extract_features(test_files)\n",
    "val_features = extract_features(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as AT\n",
    "\n",
    "class SpecTransform(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpecTransform, self).__init__()\n",
    "        sr = 22050  # ìƒ˜í”Œë§ ì£¼íŒŒìˆ˜ ì„¤ì •\n",
    "        \n",
    "        # ì‹œê°„ ë§ˆìŠ¤í‚¹ê³¼ ì£¼íŒŒìˆ˜ ë§ˆìŠ¤í‚¹ ë³€í™˜ ì •ì˜\n",
    "        self.time = AT.TimeMasking(time_mask_param=3)  # ì‹œê°„ ë§ˆìŠ¤í¬ í¬ê¸° 3\n",
    "        self.freq = AT.FrequencyMasking(freq_mask_param=5)  # ì£¼íŒŒìˆ˜ ë§ˆìŠ¤í¬ í¬ê¸° 5\n",
    "\n",
    "    def forward(self, spec):\n",
    "        # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì— ì‹œê°„ ë§ˆìŠ¤í¬ì™€ ì£¼íŒŒìˆ˜ ë§ˆìŠ¤í¬ë¥¼ ì ìš©\n",
    "        spec = self.time(spec)\n",
    "        spec = self.freq(spec)\n",
    "        return spec\n",
    "\n",
    "# ë³€í™˜ ê°ì²´ ìƒì„±\n",
    "spec = SpecTransform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜¤ë””ì˜¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ì •ì˜\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, file_paths, label, n_mfcc=60):\n",
    "        self.file_paths = file_paths  # ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ\n",
    "        self.label = label  # ë ˆì´ë¸”\n",
    "        self.n_mfcc = n_mfcc  # MFCC ì°¨ì› ì„¤ì •\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)  # ë°ì´í„°ì…‹ í¬ê¸° ë°˜í™˜\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # ì£¼ì–´ì§„ ì¸ë±ìŠ¤ì˜ ì˜¤ë””ì˜¤ íŒŒì¼ì„ ê°€ì ¸ì™€ì„œ ëœë¤ í”„ë ˆì„ì„ ì¶”ì¶œ\n",
    "        audio = self.file_paths[idx]\n",
    "        audio = np.array(audio)\n",
    "        extracted_frame = extract_random_frames(audio)  # ëœë¤ í”„ë ˆì„ ì¶”ì¶œ\n",
    "        \n",
    "        # íŠ¹ì§• ì¶”ì¶œ\n",
    "        result = feature_extract.stack_features(waveform=extracted_frame)\n",
    "        \n",
    "        # ìŠ¤í™íŠ¸ë¡œê·¸ë¨ì— ë³€í™˜ ì ìš©\n",
    "        result[1] = spec(result[1])\n",
    "        \n",
    "        # ë ˆì´ë¸” ë°˜í™˜\n",
    "        label = int(self.label[idx])\n",
    "        return result.float(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# è®­ç»ƒæ•°æ®é›†\n",
    "train_dataset = AudioDataset(train_files, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# éªŒè¯æ•°æ®é›†\n",
    "val_dataset = AudioDataset(val_files, val_labels)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)\n",
    "\n",
    "# æµ‹è¯•æ•°æ®é›†\n",
    "test_dataset = AudioDataset(test_files, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_random_frames' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# ì²« ë²ˆì§¸ ìƒ˜í”Œì˜ í¬ê¸° í™•ì¸\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mAudioDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     15\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_paths[idx]\n\u001b[0;32m     16\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(audio)\n\u001b[1;32m---> 17\u001b[0m extracted_frame \u001b[38;5;241m=\u001b[39m \u001b[43mextract_random_frames\u001b[49m(audio)  \u001b[38;5;66;03m# ëœë¤ í”„ë ˆì„ ì¶”ì¶œ\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# íŠ¹ì§• ì¶”ì¶œ\u001b[39;00m\n\u001b[0;32m     20\u001b[0m result \u001b[38;5;241m=\u001b[39m feature_extract\u001b[38;5;241m.\u001b[39mstack_features(waveform\u001b[38;5;241m=\u001b[39mextracted_frame)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_random_frames' is not defined"
     ]
    }
   ],
   "source": [
    "# æ•°æ®é›†å¤§å°æ£€æŸ¥\n",
    "train_dataset[0][0].shape  # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ ·å“çš„å¤§å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9831, 1408)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# åŸ¹è®­å’ŒéªŒè¯æ•°æ®é›†å°ºå¯¸æ£€æŸ¥\n",
    "len(train_dataset), len(val_dataset)  # åŸ¹è®­å’ŒéªŒè¯æ•°æ®é›†çš„å¤§å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_channels, reduction_ratio=4):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        \n",
    "        # í‰ê·  í’€ë§ê³¼ ìµœëŒ€ í’€ë§ì„ ì‚¬ìš©í•˜ì—¬ ì±„ë„ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # ì±„ë„ ìˆ˜ë¥¼ ì¤„ì˜€ë‹¤ê°€ ë‹¤ì‹œ í™•ì¥í•˜ëŠ” ë‘ ê°œì˜ 1x1 ì»¨ë³¼ë£¨ì…˜ì„ ì‚¬ìš©\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # í‰ê·  í’€ë§ê³¼ ìµœëŒ€ í’€ë§ì„ í†µí•´ ê° ì±„ë„ì— ëŒ€í•œ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        \n",
    "        # ê° ì±„ë„ì˜ ì¤‘ìš”ë„ë¥¼ í•©ì‚°í•˜ê³  ì…ë ¥ì— ê°€ì¤‘ì¹˜ë¥¼ ì ìš©\n",
    "        out = avg_out + max_out\n",
    "        out = out * x\n",
    "        return F.relu(out)  # ReLU í™œì„±í™” í•¨ìˆ˜ ì ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stage1, self).__init__()\n",
    "\n",
    "        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ + ë°°ì¹˜ ì •ê·œí™” + GELU í™œì„±í™” í•¨ìˆ˜\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # ì±„ë„ ì–´í…ì…˜\n",
    "        self.cam1 = ChannelAttention(64)\n",
    "        # ë§¥ìŠ¤í’€ë§\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë° ì±„ë„ ì–´í…ì…˜ ì ìš©\n",
    "        out = self.conv1(x)\n",
    "        out = self.cam1(out)\n",
    "        out = self.maxpool(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(AConvBlock, self).__init__()\n",
    "\n",
    "        # ë©”ì¸ ë¸Œëœì¹˜: ì—¬ëŸ¬ ê°œì˜ 7x7 ì»¨ë³¼ë£¨ì…˜ê³¼ ë°°ì¹˜ ì •ê·œí™”\n",
    "        self.main_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(out_channels, out_channels * 8, kernel_size=7, stride=1, padding=3),\n",
    "            nn.BatchNorm2d(out_channels * 8)\n",
    "        )\n",
    "        \n",
    "        # Shortcut: ì…ë ¥ê³¼ ì¶œë ¥ì˜ ì°¨ì›ì´ ë‹¤ë¥´ë©´ 1x1 ì»¨ë³¼ë£¨ì…˜ì„ ì‚¬ìš©í•´ ë§ì¶°ì¤Œ\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * 8:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * 8, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * 8)\n",
    "            )\n",
    "        \n",
    "        # ì±„ë„ ì–´í…ì…˜\n",
    "        self.cam = ChannelAttention(out_channels * 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ë©”ì¸ ë¸Œëœì¹˜ë¥¼ í†µê³¼í•œ ê²°ê³¼\n",
    "        out = self.main_branch(x)\n",
    "        # shortcut ì—°ê²° ì¶”ê°€\n",
    "        out += self.cam(self.shortcut(x))\n",
    "        return F.relu(out)  # ReLU í™œì„±í™” í•¨ìˆ˜ ì ìš©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        # ë„¤íŠ¸ì›Œí¬ì˜ ê° ë‹¨ê³„ ì •ì˜\n",
    "        self.conv1_x = Stage1()\n",
    "        self.conv2_x = AConvBlock(64, 32)  # ì…ë ¥ ì±„ë„ 64, ì¶œë ¥ ì±„ë„ 32\n",
    "        self.conv3_x = AConvBlock(32*8, 32)\n",
    "        self.conv4_x = AConvBlock(32*8, 32)\n",
    "        self.conv5_x = AConvBlock(32*8, 64)  # ì…ë ¥ ì±„ë„ 32*8, ì¶œë ¥ ì±„ë„ 64\n",
    "        self.conv6_x = AConvBlock(64*8, 64)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64*8, 24)\n",
    "        self.fc2 = nn.Linear(24, num_classes)\n",
    "\n",
    "        # Average pooling ë° PReLU\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1_x(x)\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.conv5_x(out)\n",
    "        out = self.conv6_x(out)\n",
    "        \n",
    "        # Average pooling í›„ flatten\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        x = self.prelu(out)\n",
    "        y = self.fc2(x)\n",
    "        \n",
    "        return x, y  # xëŠ” ì¤‘ê°„ ì¶œë ¥, yëŠ” ìµœì¢… ì¶œë ¥\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = ResNet().to(device)  # ëª¨ë¸ì„ ì§€ì •ëœ ì¥ì¹˜ë¡œ ì´ë™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenterLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Center loss.\n",
    "    \n",
    "    Reference:\n",
    "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
    "    \n",
    "    Args:\n",
    "        num_classes (int): number of classes.\n",
    "        feat_dim (int): feature dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=10, feat_dim=256, use_gpu=True):\n",
    "        super(CenterLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feat_dim = feat_dim\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        # í´ë˜ìŠ¤ ì¤‘ì‹¬ ì´ˆê¸°í™”\n",
    "        if self.use_gpu:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
    "        else:\n",
    "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: feature matrix with shape (batch_size, feat_dim).\n",
    "            labels: ground truth labels with shape (batch_size).\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # ê° í´ë˜ìŠ¤ ì¤‘ì‹¬ê³¼ì˜ ê±°ë¦¬ ê³„ì‚°\n",
    "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
    "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
    "        distmat.addmm_(1, -2, x, self.centers.t())\n",
    "\n",
    "        # ë ˆì´ë¸”ê³¼ ì¼ì¹˜í•˜ëŠ” ìœ„ì¹˜ ë§ˆìŠ¤í¬ ìƒì„±\n",
    "        classes = torch.arange(self.num_classes).long()\n",
    "        if self.use_gpu: \n",
    "            classes = classes.cuda()\n",
    "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
    "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
    "\n",
    "        # ê±°ë¦¬ ê³„ì‚° í›„ ì†ì‹¤ ê°’ êµ¬í•˜ê¸°\n",
    "        dist = distmat * mask.float()\n",
    "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam, SGD\n",
    "import torch.nn as nn\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ì™€ CenterLoss ì´ˆê¸°í™”\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # CrossEntropyLossëŠ” ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì‚¬ìš©\n",
    "center = CenterLoss(4, 24).to(device)  # 4ê°œì˜ í´ë˜ìŠ¤, feature dimension 24\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ì„¤ì •\n",
    "opti1 = Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)  # ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©\n",
    "opti2 = SGD(center.parameters(), lr=0.5)  # CenterLoss íŒŒë¼ë¯¸í„°ì— ëŒ€í•´ SGD ì‚¬ìš©\n",
    "\n",
    "# í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •\n",
    "scheduler1 = torch.optim.lr_scheduler.StepLR(opti1, step_size=10, gamma=0.5)  # 10ë²ˆì§¸ epochë§ˆë‹¤ lrì„ 0.5ë°°ë¡œ ê°ì†Œ\n",
    "scheduler2 = torch.optim.lr_scheduler.StepLR(opti2, step_size=10, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train(model, dataloader, criterion, data_len, opti1, opti2):\n",
    "    correct = 0\n",
    "    losses = 0\n",
    "\n",
    "    model.train()  # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    for data, target in tqdm(dataloader):  # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê°€ì ¸ì˜´\n",
    "        data = data.to(device)  # ë°ì´í„°ë¥¼ device(CPU/GPU)ë¡œ ì´ë™\n",
    "        target = target.to(device)  # íƒ€ê²Ÿ ë ˆì´ë¸”ì„ deviceë¡œ ì´ë™\n",
    "        \n",
    "        cen, output = model(data)  # ëª¨ë¸ì˜ ì¶œë ¥ê°’ê³¼ íŠ¹ì§• ë²¡í„° ê³„ì‚°\n",
    "        loss1 = criterion(output, target)  # CrossEntropyLoss ê³„ì‚°\n",
    "        loss2 = center(cen, target)  # CenterLoss ê³„ì‚°\n",
    "        loss = loss1 + loss2  # ë‘ ì†ì‹¤ì„ í•©ì‚°\n",
    "\n",
    "        opti1.zero_grad()  # ì˜µí‹°ë§ˆì´ì €1ì˜ ê¸°ìš¸ê¸°ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        opti2.zero_grad()  # ì˜µí‹°ë§ˆì´ì €2ì˜ ê¸°ìš¸ê¸°ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "        loss.backward()  # ì—­ì „íŒŒ ê³„ì‚°\n",
    "        opti1.step()  # ì˜µí‹°ë§ˆì´ì €1 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "        opti2.step()  # ì˜µí‹°ë§ˆì´ì €2 íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "\n",
    "        # ì˜ˆì¸¡ê°’ì„ êµ¬í•˜ê³  ì •í™•ë„ ê³„ì‚°\n",
    "        pred = output.max(1, keepdim=True)[1]  # ì˜ˆì¸¡ê°’ì˜ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()  # ì •í™•ë„ ê³„ì‚°\n",
    "        losses += loss.item()  # ì†ì‹¤ ê°’ í•©ì‚°\n",
    "\n",
    "    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì—…ë°ì´íŠ¸\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "\n",
    "    # ì •í™•ë„ì™€ í‰ê·  ì†ì‹¤ ë°˜í™˜\n",
    "    return 100 * correct / data_len, losses / data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, data_len):\n",
    "    correct = 0\n",
    "\n",
    "    model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •\n",
    "    with torch.no_grad():  # í‰ê°€ ì‹œ ê¸°ìš¸ê¸° ê³„ì‚°ì„ í•˜ì§€ ì•ŠìŒ\n",
    "        for data, target in dataloader:  # ë°ì´í„° ë¡œë”ì—ì„œ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´\n",
    "            data = data.to(device)  # ë°ì´í„°ë¥¼ deviceë¡œ ì´ë™\n",
    "            target = target.to(device)  # íƒ€ê²Ÿ ë ˆì´ë¸”ì„ deviceë¡œ ì´ë™\n",
    "\n",
    "            _, output = model(data)  # ëª¨ë¸ ì¶œë ¥ ê³„ì‚°\n",
    "            loss = criterion(output, target)  # ì†ì‹¤ ê³„ì‚°\n",
    "\n",
    "            # ì˜ˆì¸¡ê°’ì„ êµ¬í•˜ê³  ì •í™•ë„ ê³„ì‚°\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # ì •í™•ë„ ê³„ì‚°\n",
    "    acc = 100. * correct / data_len\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/77 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "epoch = 300\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    # Training the model\n",
    "    train_acc, train_loss = train(model, train_dataloader, criterion, len(train_dataloader.dataset), opti1, opti2)\n",
    "    \n",
    "    # Evaluating the model on validation data\n",
    "    val_acc = evaluate(model, val_dataloader, criterion, len(val_dataloader.dataset))\n",
    "    \n",
    "    # Uncomment the line below if you want to evaluate on test data\n",
    "    # test_acc = evaluate(model, test_dataloader, criterion, len(test_dataloader.dataset))\n",
    "\n",
    "    # Storing the accuracies\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # Printing the results for the current epoch\n",
    "    print(f\"[Epoch: {i+1}], [Validation Acc: {val_acc:.4f}]\")\n",
    "    print(f\"train_acc: {train_acc}, train_loss: {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Plotting training and validation accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracies')\n",
    "plt.show()\n",
    "\n",
    "# Setting up the device and loading the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ResNet().to(device)\n",
    "model.load_state_dict(torch.load('./deepship7.pt'))\n",
    "\n",
    "# Function to evaluate precision, recall, and F1-score\n",
    "def evaluate_metrics(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            \n",
    "            _, output = model(data)\n",
    "            pred = output.argmax(dim=1)  # Get the class with the highest score\n",
    "            \n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "\n",
    "    # Calculate precision, recall, and F1-score for each class\n",
    "    precision = precision_score(all_targets, all_preds, average=None)\n",
    "    recall = recall_score(all_targets, all_preds, average=None)\n",
    "    f1 = f1_score(all_targets, all_preds, average=None)\n",
    "\n",
    "    # Calculate average precision, recall, and F1-score\n",
    "    avg_precision = precision.mean()\n",
    "    avg_recall = recall.mean()\n",
    "    avg_f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1, avg_precision, avg_recall, avg_f1\n",
    "\n",
    "# Evaluate metrics on the test dataset\n",
    "class_precision, class_recall, class_f1, avg_precision, avg_recall, avg_f1 = evaluate_metrics(model, test_dataloader)\n",
    "\n",
    "# Print performance for each class\n",
    "for i in range(len(class_precision)):\n",
    "    print(f\"Class {i} - Precision: {class_precision[i]:.4f}, Recall: {class_recall[i]:.4f}, F1-score: {class_f1[i]:.4f}\")\n",
    "\n",
    "# Print average performance across all classes\n",
    "print(f\"Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1-score: {avg_f1:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_deepship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
