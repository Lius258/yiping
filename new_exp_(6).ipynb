{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL85Q1R26uOG",
        "outputId": "dbcfee24-c216-4269-acc3-8a534d5a75f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xPGnha6CR1HI",
        "outputId": "dff6e890-cf89-44be-d069-81b6bdad1293"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/mute&cut_cqt.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "src = \"/content/drive/MyDrive/mute&cut_cqt.zip\"  # 原文件路径\n",
        "dst = \"/content/mute&cut_cqt.zip\"  # 目标路径\n",
        "\n",
        "shutil.copy(src, dst)  # 复制文件（不包含元数据）\n",
        "# shutil.copy2(src, dst)  # 复制文件（包含元数据）"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "src = \"/content/drive/MyDrive/mute&cut_rgba.zip\"  # 原文件路径\n",
        "dst = \"/content/mute&cut_rgba.zip\"  # 目标路径\n",
        "\n",
        "shutil.copy(src, dst)  # 复制文件（不包含元数据）\n",
        "# shutil.copy2(src, dst)  # 复制文件（包含元数据）"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "03MAH2FY5fcR",
        "outputId": "14e3d030-5c82-4a79-a10a-8cbc5d5f24e8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/mute&cut_rgba.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/mute&cut_rgba.zip\"       # 待解压的 ZIP 文件路径\n",
        "extract_to = \"/content\"   # 解压后存放文件的目标文件夹\n",
        "\n",
        "# 打开 ZIP 文件并解压\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ],
      "metadata": {
        "id": "xhV1tJMb5kt6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_path = \"/content/mute&cut_cqt.zip\"       # 待解压的 ZIP 文件路径\n",
        "extract_to = \"/content\"   # 解压后存放文件的目标文件夹\n",
        "\n",
        "# 打开 ZIP 文件并解压\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)"
      ],
      "metadata": {
        "id": "fC7xU0kF5whM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LvkOtkQG6oqR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPZa_aXIr-0a",
        "outputId": "1828ed72-2cea-4deb-c711-bbdfc2ea4206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "总共找到的数据对: 11326\n",
            "总共找到的数据对: 2721\n",
            "训练集大小: 11326\n",
            "测试集大小: 2721\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class GTCCMultiModalDataset(Dataset):\n",
        "    def __init__(self, rgba_dir, cqt_dir, split='train', transform_resnet=None, transform_efficientnet=None):\n",
        "        \"\"\"\n",
        "        自定义数据集类，确保 RGBA (4 通道) 和 CQT (1 通道) 数据的文件名和标签匹配。\n",
        "\n",
        "        参数:\n",
        "            rgba_dir (str): RGBA 图像数据集根目录，例如 \"full_mixed_rgba\"。\n",
        "            cqt_dir (str): CQT 频谱图像数据集根目录，例如 \"full_mixed_cqt\"。\n",
        "            split (str): 'train' 或 'test' 指定加载的子目录。\n",
        "            transform_resnet: 适用于 RGBA 的图像转换 (ToTensor, Normalize)。\n",
        "            transform_efficientnet: 适用于 CQT 的图像转换 (ToTensor, Normalize)。\n",
        "        \"\"\"\n",
        "        self.transform_resnet = transform_resnet\n",
        "        self.transform_efficientnet = transform_efficientnet\n",
        "\n",
        "        rgba_split_dir = os.path.join(rgba_dir, split)\n",
        "        cqt_split_dir = os.path.join(cqt_dir, split)\n",
        "\n",
        "        # 获取所有类别\n",
        "        self.classes = sorted([d for d in os.listdir(rgba_split_dir) if os.path.isdir(os.path.join(rgba_split_dir, d))])\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
        "\n",
        "        self.data_pairs = []\n",
        "        # 遍历 RGBA 数据集，并确保匹配 CQT 频谱数据\n",
        "        for class_name in self.classes:\n",
        "            rgba_class_dir = os.path.join(rgba_split_dir, class_name)\n",
        "            cqt_class_dir = os.path.join(cqt_split_dir, class_name)\n",
        "\n",
        "            for root, _, files in os.walk(rgba_class_dir):\n",
        "                for fname in files:\n",
        "                    if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        rgba_img_path = os.path.join(root, fname)\n",
        "                        cqt_img_path = os.path.join(cqt_class_dir, os.path.relpath(rgba_img_path, rgba_class_dir))  # 假设文件名相同\n",
        "\n",
        "                        if os.path.exists(cqt_img_path):  # 确保 CQT 文件存在\n",
        "                            label = self.class_to_idx[class_name]\n",
        "                            self.data_pairs.append((rgba_img_path, cqt_img_path, label))\n",
        "                        else:\n",
        "                            print(f\"CQT 图像不存在: {cqt_img_path}\")\n",
        "                    else:\n",
        "                        print(f\"RGBA 目录中发现无效图像文件: {os.path.join(root, fname)}\")\n",
        "\n",
        "        print(f\"总共找到的数据对: {len(self.data_pairs)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgba_path, cqt_path, label = self.data_pairs[idx]\n",
        "\n",
        "        # 加载 RGBA 图像\n",
        "        rgba_image = Image.open(rgba_path).convert('RGBA')  # 确保 4 通道\n",
        "        if self.transform_resnet:\n",
        "            rgba_image = self.transform_resnet(rgba_image)\n",
        "\n",
        "        # 加载 CQT 频谱图像\n",
        "        cqt_image = Image.open(cqt_path).convert('L')  # 确保 1 通道\n",
        "        if self.transform_efficientnet:\n",
        "            cqt_image = self.transform_efficientnet(cqt_image)\n",
        "\n",
        "        # 处理标签\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return rgba_image, cqt_image, label\n",
        "\n",
        "\n",
        "# 示例使用\n",
        "if __name__ == '__main__':\n",
        "    from torchvision import transforms\n",
        "\n",
        "    # 设置数据集路径\n",
        "    rgba_dir = \"/content/mute&cut_rgba\"\n",
        "    cqt_dir = \"/content/mute&cut_cqt\"\n",
        "\n",
        "    # RGBA 图像的 transforms\n",
        "    transform_resnet = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5))  # RGBA 4 通道\n",
        "    ])\n",
        "\n",
        "    # CQT 频谱图像的 transforms\n",
        "    transform_efficientnet = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # CQT 1 通道\n",
        "    ])\n",
        "\n",
        "    # 创建训练集\n",
        "    train_dataset = GTCCMultiModalDataset(rgba_dir, cqt_dir, split='train',\n",
        "                                          transform_resnet=transform_resnet,\n",
        "                                          transform_efficientnet=transform_efficientnet)\n",
        "\n",
        "    # 创建测试集\n",
        "    test_dataset = GTCCMultiModalDataset(rgba_dir, cqt_dir, split='test',\n",
        "                                         transform_resnet=transform_resnet,\n",
        "                                         transform_efficientnet=transform_efficientnet)\n",
        "\n",
        "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
        "        print(\"未找到数据。请检查您的数据集目录和文件名。\")\n",
        "    else:\n",
        "        # 创建 DataLoader\n",
        "        batch_size = 128\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        print(f\"训练集大小: {len(train_dataset)}\")\n",
        "        print(f\"测试集大小: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import glob\n",
        "\n",
        "class GTCCMultiModalDataset(Dataset):\n",
        "    def __init__(self, rgba_dir, cqt_dir, split='train',\n",
        "                 transform_resnet=None, transform_efficientnet=None):\n",
        "        \"\"\"\n",
        "        自定义数据集类，支持在更深层级(三级目录及以下)查找图像\n",
        "        \"\"\"\n",
        "        self.transform_resnet = transform_resnet\n",
        "        self.transform_efficientnet = transform_efficientnet\n",
        "\n",
        "        rgba_split_dir = os.path.join(rgba_dir, split)\n",
        "        cqt_split_dir = os.path.join(cqt_dir, split)\n",
        "\n",
        "        # 首先获取\"类别\" (依然假设类别就是二级目录名字)\n",
        "        self.classes = sorted([\n",
        "            d for d in os.listdir(rgba_split_dir)\n",
        "            if os.path.isdir(os.path.join(rgba_split_dir, d))\n",
        "        ])\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.classes)}\n",
        "\n",
        "        # 开始递归地遍历\n",
        "        self.data_pairs = []\n",
        "        for class_name in self.classes:\n",
        "            rgba_class_dir = os.path.join(rgba_split_dir, class_name)\n",
        "            cqt_class_dir = os.path.join(cqt_split_dir, class_name)\n",
        "\n",
        "            # **使用glob进行递归搜索**，这里示例只搜 '.png', '.jpg', '.jpeg'\n",
        "            patterns = ['*.png', '*.jpg', '*.jpeg']\n",
        "            rgba_file_list = []\n",
        "            for p in patterns:\n",
        "                rgba_file_list.extend(glob.glob(os.path.join(rgba_class_dir, '**', p), recursive=True))\n",
        "\n",
        "            # 开始匹配这些RGBA文件对应的CQT文件\n",
        "            for rgba_img_path in rgba_file_list:\n",
        "                # 拿到最末级文件名\n",
        "                fname = os.path.basename(rgba_img_path)\n",
        "                # 构造对应的 CQT 文件路径 (假设同名)\n",
        "                cqt_img_path = os.path.join(cqt_class_dir, fname)\n",
        "\n",
        "                if os.path.exists(cqt_img_path):\n",
        "                    label = self.class_to_idx[class_name]\n",
        "                    self.data_pairs.append((rgba_img_path, cqt_img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        rgba_path, cqt_path, label = self.data_pairs[idx]\n",
        "\n",
        "        # 加载 RGBA\n",
        "        rgba_image = Image.open(rgba_path).convert('RGBA')\n",
        "        if self.transform_resnet:\n",
        "            rgba_image = self.transform_resnet(rgba_image)\n",
        "\n",
        "        # 加载 CQT\n",
        "        cqt_image = Image.open(cqt_path).convert('L')\n",
        "        if self.transform_efficientnet:\n",
        "            cqt_image = self.transform_efficientnet(cqt_image)\n",
        "\n",
        "        label = torch.tensor(label, dtype=torch.long)\n",
        "        return rgba_image, cqt_image, label\n"
      ],
      "metadata": {
        "id": "W2qjJ30EEGBR"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "q5hUpGnjF61d"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=4):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        # 평균 풀링과 최대 풀링을 사용하여 채널 중요도 계산\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # 채널 수를 줄였다가 다시 확장하는 두 개의 1x1 컨볼루션을 사용\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 평균 풀링과 최대 풀링을 통해 각 채널에 대한 중요도를 계산\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "\n",
        "        # 각 채널의 중요도를 합산하고 입력에 가중치를 적용\n",
        "        out = avg_out + max_out\n",
        "        out = out * x\n",
        "        return F.relu(out)  # ReLU 활성화 함수 적용\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "xHpwfbVjGAVf"
      },
      "outputs": [],
      "source": [
        "class Stage1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stage1, self).__init__()\n",
        "\n",
        "        # 첫 번째 컨볼루션 + 배치 정규화 + GELU 활성화 함수\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(4, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # 채널 어텐션\n",
        "        self.cam1 = ChannelAttention(64)\n",
        "        # 맥스풀링\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 첫 번째 컨볼루션 및 채널 어텐션 적용\n",
        "        out = self.conv1(x)\n",
        "        out = self.cam1(out)\n",
        "        out = self.maxpool(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "OIA0--JMGAKt"
      },
      "outputs": [],
      "source": [
        "class AConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(AConvBlock, self).__init__()\n",
        "\n",
        "        # 메인 브랜치: 여러 개의 7x7 컨볼루션과 배치 정규화\n",
        "        self.main_branch = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=stride, padding=3),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=7, stride=1, padding=3),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(out_channels, out_channels * 8, kernel_size=7, stride=1, padding=3),\n",
        "            nn.BatchNorm2d(out_channels * 8)\n",
        "        )\n",
        "\n",
        "        # Shortcut: 입력과 출력의 차원이 다르면 1x1 컨볼루션을 사용해 맞춰줌\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * 8:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * 8, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * 8)\n",
        "            )\n",
        "\n",
        "        # 채널 어텐션\n",
        "        self.cam = ChannelAttention(out_channels * 8)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 메인 브랜치를 통과한 결과\n",
        "        out = self.main_branch(x)\n",
        "        # shortcut 연결 추가\n",
        "        out += self.cam(self.shortcut(x))\n",
        "        return F.relu(out)  # ReLU 활성화 함수 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "P52XX0kGGisH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetFeatureExtractor, self).__init__()\n",
        "\n",
        "        # 定义网络的各个阶段\n",
        "        self.conv1_x = Stage1()  # 第一个卷积层和池化层\n",
        "        self.conv2_x = AConvBlock(64, 32)  # 输入通道64，输出通道32\n",
        "        self.conv3_x = AConvBlock(32, 64)  # 输入通道32，输出通道64\n",
        "        self.conv4_x = AConvBlock(64, 128) # 输入通道64，输出通道128\n",
        "        self.conv5_x = AConvBlock(128, 256) # 输入通道128，输出通道256\n",
        "        self.conv6_x = AConvBlock(256, 512) # 输入通道256，输出通道512\n",
        "\n",
        "        # 全连接层，输出维度为512\n",
        "        self.fc = nn.Linear(512, 512)\n",
        "\n",
        "        # 自适应平均池化和PReLU激活函数\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 检查输入张量的维度\n",
        "        if len(x.size()) != 4:\n",
        "            raise ValueError(f\"Expected input tensor to have 4 dimensions, got {len(x.size())}\")\n",
        "\n",
        "        # 前向传播过程\n",
        "        out = self.conv1_x(x)\n",
        "        out = self.conv2_x(out)\n",
        "        out = self.conv3_x(out)\n",
        "        out = self.conv4_x(out)\n",
        "        out = self.conv5_x(out)\n",
        "        out = self.conv6_x(out)\n",
        "\n",
        "        # 自适应平均池化后展平\n",
        "        out = self.avgpool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # 全连接层，输出512维特征\n",
        "        out = self.fc(out)\n",
        "        out = self.prelu(out)\n",
        "\n",
        "        return out  # 输出512维特征\n",
        "\n",
        "# Stage1的定义，包括第一个卷积层、批归一化层、ReLU激活函数和最大池化层\n",
        "class Stage1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Stage1, self).__init__()\n",
        "        self.conv = nn.Conv2d(4, 64, kernel_size=7, stride=2, padding=3, bias=False)  # 修改输入通道数为4\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        return x\n",
        "\n",
        "# AConvBlock的定义，包括一个卷积层、批归一化层和ReLU激活函数\n",
        "class AConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(AConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "qpXEEoAeem4N"
      },
      "outputs": [],
      "source": [
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channels // reduction, channels, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.global_avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_attention = SEBlock(channels, reduction)\n",
        "        self.spatial_attention = nn.Sequential(\n",
        "            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.channel_attention(x)\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        y = torch.cat([avg_out, max_out], dim=1)\n",
        "        y = self.spatial_attention(y)\n",
        "        return x * y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "smLX1thQe0Jm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiModalModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(MultiModalModel, self).__init__()\n",
        "\n",
        "        # **1. ResNet 处理 RGBA Mel 频谱**\n",
        "        self.resnet = ResNetFeatureExtractor()  # **只接受 RGBA 4 通道输入**\n",
        "        self.resnet_cbam = CBAM(512)  # **CBAM 适配 ResNet 512 维特征**\n",
        "\n",
        "        # **2. EfficientNet-B5 处理 CQT 频谱**\n",
        "        self.efficientnet = models.efficientnet_b5(pretrained=True)\n",
        "        self.efficientnet.features[0][0] = nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1, bias=False)  # **只接受 CQT 1 通道输入**\n",
        "        self.efficientnet.classifier = nn.Identity()  # **移除分类层**\n",
        "        self.efficientnet_cbam = CBAM(2048)  # **CBAM 适配 EfficientNet 2048 维特征**\n",
        "\n",
        "        # **3. 降维 & 融合层**\n",
        "        self.conv1_resnet = nn.Conv2d(512, 512, kernel_size=1)  # **ResNet 降维**\n",
        "        self.conv1_efficientnet = nn.Conv2d(2048, 512, kernel_size=1)  # **EfficientNet 降维**\n",
        "\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # **全局池化**\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 + 512, 512),  # **最终特征融合**\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x_resnet, x_cqt):\n",
        "        \"\"\"\n",
        "        x_resnet: RGBA (4通道) 只输入 ResNet\n",
        "        x_cqt: CQT (1通道) 只输入 EfficientNet\n",
        "        \"\"\"\n",
        "        # **ResNet 处理 RGBA**\n",
        "        resnet_feat = self.resnet(x_resnet)  # (batch, 512, 7, 7)\n",
        "        resnet_feat = self.resnet_cbam(resnet_feat)\n",
        "        resnet_feat = self.conv1_resnet(resnet_feat)\n",
        "\n",
        "        # **EfficientNet 处理 CQT**\n",
        "        efficientnet_feat = self.efficientnet(x_cqt)  # (batch, 2048, 7, 7)\n",
        "        efficientnet_feat = self.efficientnet_cbam(efficientnet_feat)\n",
        "        efficientnet_feat = self.conv1_efficientnet(efficientnet_feat)\n",
        "\n",
        "        # **全局平均池化**\n",
        "        resnet_feat = self.global_pool(resnet_feat).view(resnet_feat.size(0), -1)  # (batch, 512)\n",
        "        efficientnet_feat = self.global_pool(efficientnet_feat).view(efficientnet_feat.size(0), -1)  # (batch, 512)\n",
        "\n",
        "        # **特征融合**\n",
        "        fusion = torch.cat((resnet_feat, efficientnet_feat), dim=1)  # (batch, 1024)\n",
        "\n",
        "        # **最终分类**\n",
        "        output = self.fc(fusion)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Q4ry7orOGij6"
      },
      "outputs": [],
      "source": [
        "# 设置计算设备（GPU/CPU）\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = MultiModalModel().to(device)  # 모델을 지정된 장치로 이동"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "-2pkJUDeGicL"
      },
      "outputs": [],
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Center loss.\n",
        "\n",
        "    Reference:\n",
        "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        feat_dim (int): feature dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, feat_dim=256, use_gpu=True):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # 클래스 중심 초기화\n",
        "        if self.use_gpu:\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
        "        else:\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # 각 클래스 중심과의 거리 계산\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(1, -2, x, self.centers.t())\n",
        "\n",
        "        # 레이블과 일치하는 위치 마스크 생성\n",
        "        classes = torch.arange(self.num_classes).long()\n",
        "        if self.use_gpu:\n",
        "            classes = classes.cuda()\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        "\n",
        "        # 거리 계산 후 손실 값 구하기\n",
        "        dist = distmat * mask.float()\n",
        "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
        "\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "x4gY9uIzJbUG"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam, SGD\n",
        "import torch.nn as nn\n",
        "\n",
        "# **修改 CenterLoss 维度为 512**\n",
        "criterion = nn.CrossEntropyLoss().to(device)  # 交叉熵损失 (分类任务)\n",
        "center = CenterLoss(4, 1024).to(device)  # **修改 feature dimension 24 -> 512**\n",
        "\n",
        "# **优化器**\n",
        "opti1 = Adam(model.parameters(), lr=1e-4, weight_decay=5e-4)  # **模型参数**\n",
        "opti2 = SGD(center.parameters(), lr=0.2)  # **降低 CenterLoss 学习率**\n",
        "\n",
        "# **学习率调度器**\n",
        "scheduler1 = torch.optim.lr_scheduler.StepLR(opti1, step_size=10, gamma=0.5)  # **每 10 轮 lr * 0.5**\n",
        "scheduler2 = torch.optim.lr_scheduler.StepLR(opti2, step_size=10, gamma=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "tAxwYL9nJ0re"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, dataloader, criterion, data_len, opti1, opti2):\n",
        "    correct = 0\n",
        "    losses = 0\n",
        "\n",
        "    model.train()  # 设置模型为训练模式\n",
        "    for rgba_data, cqt_data, target in tqdm(dataloader):  # RGBA & CQT 数据\n",
        "        # 将数据移动到 GPU 或 CPU\n",
        "        rgba_data = rgba_data.to(device)\n",
        "        cqt_data = cqt_data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # **模型前向传播**\n",
        "        cen, output = model(rgba_data, cqt_data)  # **使用两个输入**\n",
        "\n",
        "        # **计算损失**\n",
        "        loss1 = criterion(output, target)  # 交叉熵损失\n",
        "        loss2 = center(cen, target)  # CenterLoss\n",
        "        loss = loss1 + loss2  # **融合损失**\n",
        "\n",
        "        # **优化器梯度清零**\n",
        "        opti1.zero_grad()\n",
        "        opti2.zero_grad()\n",
        "\n",
        "        # **反向传播**\n",
        "        loss.backward()\n",
        "        opti1.step()\n",
        "        opti2.step()\n",
        "\n",
        "        # **计算准确率**\n",
        "        pred = output.max(1, keepdim=True)[1]  # 取最大值索引\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()  # 计算正确预测的数量\n",
        "        losses += loss.item()\n",
        "\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "    # **返回准确率和平均损失**\n",
        "    return 100 * correct / data_len, losses / data_len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "Bmn_AXdcJ66R"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, criterion, data_len):\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    model.eval()  # **设置模型为评估模式**\n",
        "    with torch.no_grad():  # **评估时不计算梯度**\n",
        "        for rgba_data, cqt_data, target in dataloader:  # **获取 RGBA & CQT 数据**\n",
        "            rgba_data = rgba_data.to(device)\n",
        "            cqt_data = cqt_data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # **前向传播**\n",
        "            _, output = model(rgba_data, cqt_data)\n",
        "\n",
        "            # **计算损失**\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # **计算准确率**\n",
        "            pred = output.max(1, keepdim=True)[1]  # 取最大值索引\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # **计算最终评估结果**\n",
        "    acc = 100. * correct / data_len\n",
        "\n",
        "    return acc  # **返回准确率 & 平均损失**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "GE_Gs-lXJ610",
        "outputId": "e2ff8778-8274-4c71-e3cc-7d6464229c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/89 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-9d8cbe612bf2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Training the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopti1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopti2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Evaluating the model on validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-3e2cbeae8b12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, criterion, data_len, opti1, opti2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# **模型前向传播**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mcen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgba_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcqt_data\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# **使用两个输入**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# **计算损失**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-a23147626f3c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_resnet, x_cqt)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# **ResNet 处理 RGBA**\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mresnet_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_resnet\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch, 512, 7, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mresnet_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_cbam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mresnet_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1_resnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-e14bd40d3876>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mavg_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mmax_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-e14bd40d3876>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_avg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
          ]
        }
      ],
      "source": [
        "epoch = 100\n",
        "\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "for i in range(epoch):\n",
        "    # Training the model\n",
        "    train_acc, train_loss = train(model, train_loader, criterion, len(train_loader.dataset), opti1, opti2)\n",
        "\n",
        "    # Evaluating the model on validation data\n",
        "    val_acc = evaluate(model, test_loader, criterion, len(test_loader.dataset))\n",
        "\n",
        "    # Uncomment the line below if you want to evaluate on test data\n",
        "    # test_acc = evaluate(model, test_dataloader, criterion, len(test_dataloader.dataset))\n",
        "\n",
        "    # Storing the accuracies\n",
        "    train_accuracies.append(train_acc)\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Printing the results for the current epoch\n",
        "    print(f\"[Epoch: {i+1}], [Validation Acc: {val_acc:.4f}]\")\n",
        "    print(f\"train_acc: {train_acc}, train_loss: {train_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFwMcU5bJ6oO"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Plotting training and validation accuracies\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_accuracies, label='Training Accuracy')\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Accuracies')\n",
        "plt.show()\n",
        "\n",
        "# Setting up the device and loading the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNet().to(device)\n",
        "model.load_state_dict(torch.load('./deepship7.pt'))\n",
        "\n",
        "# Function to evaluate precision, recall, and F1-score\n",
        "def evaluate_metrics(model, dataloader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in dataloader:\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            _, output = model(data)\n",
        "            pred = output.argmax(dim=1)  # Get the class with the highest score\n",
        "\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_targets.extend(target.cpu().numpy())\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for each class\n",
        "    precision = precision_score(all_targets, all_preds, average=None)\n",
        "    recall = recall_score(all_targets, all_preds, average=None)\n",
        "    f1 = f1_score(all_targets, all_preds, average=None)\n",
        "\n",
        "    # Calculate average precision, recall, and F1-score\n",
        "    avg_precision = precision.mean()\n",
        "    avg_recall = recall.mean()\n",
        "    avg_f1 = f1.mean()\n",
        "\n",
        "    return precision, recall, f1, avg_precision, avg_recall, avg_f1\n",
        "\n",
        "# Evaluate metrics on the test dataset\n",
        "class_precision, class_recall, class_f1, avg_precision, avg_recall, avg_f1 = evaluate_metrics(model, test_dataloader)\n",
        "\n",
        "# Print performance for each class\n",
        "for i in range(len(class_precision)):\n",
        "    print(f\"Class {i} - Precision: {class_precision[i]:.4f}, Recall: {class_recall[i]:.4f}, F1-score: {class_f1[i]:.4f}\")\n",
        "\n",
        "# Print average performance across all classes\n",
        "print(f\"Avg Precision: {avg_precision:.4f}, Avg Recall: {avg_recall:.4f}, Avg F1-score: {avg_f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}